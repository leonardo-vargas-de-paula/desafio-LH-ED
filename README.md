# Orquestra√ß√£o de pipeline com Apache Airflow

## Sum√°rio
1.[An√°lises Iniciais](#-an√°lises-iniciais)

2.[Tecnologias Utilizadas](#tecnologias-utilizadas)



[terminar depois]

# üìä An√°lises Iniciais

Este projeto √© um desafio pr√°tico do processo seletivo para o programa **Indicium Lighthouse**

O objetivo √© estabelecer um fluxo de extra√ß√£o de dados de um banco fiticio "Banvic" no formato CSV e SQL e carregar os dados em um banco PostgreSQL.

# Tecnologias Utilizadas

- **Apache Airflow 3.0.6**
- **Python/Pandas**
- **Docker**

# Pr√©-requisitos

Para o funcionamento adequado √© necess√°rio ter instalado:

- **Docker**
- **Docker Compose**
- **\*Git (caso clone o reposit√≥rio)**

# Passo-a-passo de execu√ß√£o

 1. Para executar o projeto √© necess√°rio realizar o **download** do reposit√≥rio ou    clonar com o seguinte comando no diret√≥rio desejado:

    ```sh
    git clone https://github.com/leonardo-vargas-de-paula/desafio-LH-ED.git
    ```
    Em seguida entre no diret√≥rio correspondente ao **root** do projeto *desafio-LH-ED*

 2. Inicialize o Apache Airflow na sua primeira execu√ß√£o:

    ```
    docker-compose up airflow-init
    ```
 3. Inicializar os servi√ßos:
    ```sh
    docker-compose up -d
    ```
    OBS: √â importante utilizar o comando **-d** para evitar poss√≠veis travamentos no terminal.

 4. Entre no webserver do Airflow na URL  ``http://localhost:8080`` com login ``airflow`` e senha ``airflow``


5. Configure as conex√µes referentes ao banco de dados do Banvic e o Datawarehouse de destino dentro do webserver:

    5.1.

    5.2.

    5.3.

    5.4.

    5.5.
    

