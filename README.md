# ğŸš€ OrquestraÃ§Ã£o de Pipeline com Apache Airflow

![Airflow](https://img.shields.io/badge/Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Postgres](https://img.shields.io/badge/Postgres-4169E1?style=for-the-badge&logo=postgresql&logoColor=white)

## ğŸ“‘ SumÃ¡rio  
1. [AnÃ¡lises Iniciais](#-anÃ¡lises-iniciais)  
2. [Tecnologias Utilizadas](#-tecnologias-utilizadas)  
3. [PrÃ©-requisitos](#-prÃ©-requisitos)  
4. [Estrutura do Projeto](#-estrutura-do-projeto)  
5. [LÃ³gica do Fluxo de Dados](#-lÃ³gica-do-fluxo-de-dados)  
6. [Processos da DAG](#-processos-da-dag)  
7. [Passo-a-passo de ExecuÃ§Ã£o](#passo-a-passo-de-execuÃ§Ã£o)  



------------------------------------------------------------------------

# ğŸ“Š AnÃ¡lises Iniciais

Este projeto Ã© um desafio prÃ¡tico do processo seletivo para o programa
**Indicium Lighthouse**.

ğŸ¯ **Objetivo:** Estabelecer um fluxo de extraÃ§Ã£o de dados de um banco
fictÃ­cio **Banvic** (arquivos CSV e SQL) e carregar os dados em um banco
**PostgreSQL**, orquestrado pelo **Apache Airflow**.

------------------------------------------------------------------------

# ğŸ›  Tecnologias Utilizadas

-   ğŸŒ€ **Apache Airflow 3.0.6**\
-   ğŸ **Python/Pandas**\
-   ğŸ³ **Docker & Docker Compose**

------------------------------------------------------------------------

# âš™ PrÃ©-requisitos

Antes de rodar o projeto, Ã© necessÃ¡rio ter:

-   ğŸ“¦ **Docker**
-   ğŸ“¦ **Docker Compose**
-   ğŸŒ **Acesso Ã  internet**
-   ğŸ–¥ï¸ **Git** (se optar por clonar o repositÃ³rio)
-   ğŸ›  **DBeaver** *(opcional, apenas para visualizar o banco)*

------------------------------------------------------------------------

# ğŸ—‚ Estrutura do Projeto
```
desafio-LH-ED/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ processamento_dados_banvic.py
|   â””â”€â”€ ...
â”‚
â”œâ”€â”€ raw_data/
â”‚   â”œâ”€â”€ transacoes.csv
â”‚   â””â”€â”€ banvic.sql
â”‚
â”œâ”€â”€ processed_data/
â”‚   â””â”€â”€ [data de execuÃ§Ã£o]/
â”‚                 â”œâ”€â”€ csv/
â”‚                 â””â”€â”€ sql/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ dbdata/    
â”‚   â””â”€â”€ ...
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ img/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE



```

------------------------------------------------------------------------

# ğŸ”„ LÃ³gica do Fluxo de Dados

O fluxo Ã© realizado atravÃ©s do pipeline orquestrado pelo Apache Airflow. 

O processo ocorre 4:35 da manhÃ£ todos os dias atravÃ©s da DAG ````processamento_dados_banvic````.

Os dados saem da fonte que sÃ£o um arquivo CSV bruto e um banco de dados PostgreSQL disponÃ­veis no diretorio *raw_data*. 

Toda 4:35 da manhÃ£ o pipeline extrai os dados e cria um diretÃ³rio novo dentro de *processed_data* contendo o dia em que a DAG rodou e divindo entre *csv* para os arquivos extraÃ­dos e *sql* para as tabelas retiradas   dobanco de dados e transformados em CSV. As pastas  do data lake ficam no seguinte formato:
```
[ano-mÃªs-dia] / [fonte de dados] /[nome tabela ou csv].csv
```
------------------------------------------------------------------------

# ğŸ“‚ Processos da DAG

A DAG Ã© composta pelas seguintes *tasks*
<table>
  <tr>
    <th>Task</th>
    <th>DescriÃ§Ã£o</th>
  </tr>
  <tr>
    <td>ğŸ—‚ <b>define_pastas_destino</b></td>
    <td>Cria os diretÃ³rios de saÃ­da para os dados processados</td>
  </tr>
  <tr>
    <td>ğŸ“‘ <b>processar_csv_transacoes</b></td>
    <td>Processa e organiza os arquivos CSV brutos</td>
  </tr>
  <tr>
    <td>ğŸ—„ <b>processar_tabelas_banvic</b></td>
    <td>Extrai tabelas do banco Banvic e converte para CSV</td>
  </tr>
  <tr>
    <td>â¬† <b>carregar_csv_dw_banvic</b></td>
    <td>Carrega dados CSV no Data Warehouse</td>
  </tr>
  <tr>
    <td>â¬† <b>carregar_sql_dw_banvic</b></td>
    <td>Carrega dados SQL no Data Warehouse</td>
  </tr>
</table>


# â–¶ Passo-a-passo de ExecuÃ§Ã£o

# Passo-a-passo de execuÃ§Ã£o

 1. Para executar o projeto Ã© necessÃ¡rio realizar o **download** do repositÃ³rio ou    clonar com o seguinte comando no diretÃ³rio desejado:

    ```sh
    git clone https://github.com/leonardo-vargas-de-paula/desafio-LH-ED.git
    ```
    Em seguida entre no diretÃ³rio correspondente ao **root** do projeto *desafio-LH-ED*

 2. Inicialize o Apache Airflow na sua primeira execuÃ§Ã£o:

    ```
    docker-compose up airflow-init
    ```
 3. Inicializar os serviÃ§os:
    ```sh
    docker-compose up -d
    ```
    OBS: Ã‰ importante utilizar o comando **-d** para evitar possÃ­veis travamentos no terminal.

 4. Entre no webserver do Airflow na URL  ``http://localhost:8080`` com login ``airflow`` e senha ``airflow``


5.  Configure as conexÃµes referentes ao banco de dados do Banvic e o Datawarehouse de destino dentro do webserver:
    
       5.1. Localize o botÃ£o de configuraÃ§Ãµes de admin:
        
       <div align="start">
          <img src="img\config.png" width="10%"><br>
       </div>
    
       5.2.Selecione a opÃ§Ã£o *Connections*

       <div align="start">
          <img src="img\connections.png" width="10%"><br>
       </div>
       
       5.3.Clique em *Add Connection*
       
       <div align="start">
          <img src="img\addconnection.png" width="20%"><br>
       
       </div>

       5.4. No campo *Connection ID* escreva **banvic_source_db** para identificar o ID do banco de dados do Banvic e *connection type* igual a **Postgres**.
                                          
       <div align="start">
          <img src="img\dbbanvic1.png" width="30%"><br>
       </div>
    
       5.5. Nos campos das configuraÃ§Ãµes insira as seguintes credenciais:

      <table> 
      <tr> <th>Campo</th> <th>Valor</th> </tr> 
      <tr> <td>Description</td> <td>db banvic</td> </tr> 
      <tr> <td>Host</td> <td>db</td> </tr> 
      <tr> <td>Login</td> <td>data_engineer</td> </tr> 
      <tr> <td>Password</td> <td>v3rysecur&pas5w0rd</td> </tr> 
      <tr> <td>Port</td> <td>5432</td> </tr> <tr> <td>Database</td> 
      <td>banvic</td> </tr> 
      </table>   

       <div align="start">
          <img src="img\credenciaisbanvic.png" width="30%"><br>
       </div>

       ApÃ³s isso clique em *save*

       5.6. De forma anÃ¡loga, crie a conexÃ£o com ID **banvic_dw** e *connection type* igual a **Postgres**
       
       <div align="start">
          <img src="img\dwbanvic1.png" width="45%"><br>
       </div>

      5.7. Nos campos das configuraÃ§Ãµes insira as seguintes credenciais:

       <table> <tr> <th>Campo</th> <th>Valor</th> </tr> 
       <tr> <td>Description</td> 
       <td>dw banvic</td> </tr> <tr> <td>Host</td> <td>dw_local</td> 
       </tr> <tr> <td>Login</td> <td>data_engineer</td> </tr> 
       <tr> <td>Password</td> <td>v3rysecur&pas5w0rd</td> </tr> 
       <tr> <td>Port</td> <td>5432</td> </tr> 
       <tr> <td>Database</td> <td>dw_banvic</td> </tr> 
       </table>

       <div align="start">
          <img src="img\credenciaisbanvicdw.png" width="45%"><br>
       </div>
      
      E por fim, clique em *save*. Dessa forma a **DAG** pode funcionar normalmente. Mas antes disso, serÃ¡ necessÃ¡rio ativÃ¡-la.

   6. No menu lateral, clique no botÃ£o *DAGS*:

      <div align="start">
      <img src="img\botaodags.png" width="20%"><br>
      </div>

      6.1.Realize uma busca pela DAG chamada    
      ````processamento_dados_banvic````

   7. Ao encontrar a DAG desejada, clique nela para ter um overview e verificar suas caracterÃ­sticas.

   [adicionar imagem overview]

   8. Clique no switch para ativar a DAG

   [adicionar imagem switch]

   9.  Pronto! A DAG esta agendada para rodar as 4:35 da manhÃ£. Caso deseje rodar manualmente, clique em *trigger*.

   [adicionar imagem botÃ£o trigger]
