# OrquestraÃ§Ã£o de pipeline com Apache Airflow

## SumÃ¡rio
1.[AnÃ¡lises Iniciais](#-anÃ¡lises-iniciais)

2.[Tecnologias Utilizadas](#tecnologias-utilizadas)

3.[Passo-a-passo de ExecuÃ§Ã£o](#passo-a-passo-de-execuÃ§Ã£o)



[terminar depois]

---

# ðŸ“Š AnÃ¡lises Iniciais

Este projeto Ã© um desafio prÃ¡tico do processo seletivo para o programa **Indicium Lighthouse**

O objetivo Ã© estabelecer um fluxo de extraÃ§Ã£o de dados de um banco fiticio "Banvic" no formato CSV e SQL e carregar os dados em um banco PostgreSQL.

# Tecnologias Utilizadas

- **Apache Airflow 3.0.6**
- **Python/Pandas**
- **Docker**

# PrÃ©-requisitos

Para o funcionamento adequado Ã© necessÃ¡rio ter:

- **Docker**
- **Docker Compose**
- **\*Git (caso clone o repositÃ³rio)**
- **Acesso a internet**
- **\*Dbeaver (opcional para ver o banco de dados)**

# Diagrama do fluxo de dados

<div align="start">
          <img src="img\image1.png" width="80%"><br> </div>
          Fonte: Autores do Desafio Indicium LH.

# LÃ³gica do fluxo de dados

O fluxo Ã© realizado atravÃ©s do pipeline orquestrado pelo Apache Airflow. 

O processo ocorre 4:35 da manhÃ£ todos os dias atravÃ©s da DAG ````processamento_dados_banvic````.

Os dados saem da fonte que sÃ£o um arquivo CSV bruto e um banco de dados PostgreSQL disponÃ­veis no diretorio *raw_data*. 

Toda 4:35 da manhÃ£ o pipeline extrai os dados e cria um diretÃ³rio novo dentro de *processed_data* contendo o dia em que a DAG rodou e divindo entre *csv* para os arquivos extraÃ­dos e *sql* para as tabelas retiradas   dobanco de dados e transformados em CSV. As pastas  do data lake ficam no seguinte formato:
```
[ano-mÃªs-dia] / [fonte de dados] /[nome tabela ou csv].csv*
```

# DiretÃ³rio base do projeto

```
desafio-LH-ED/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ processamento_dados_banvic.py
|   â””â”€â”€ ...
â”‚
â”œâ”€â”€ raw_data/
â”‚   â”œâ”€â”€ transacoes.csv
â”‚   â””â”€â”€ banvic.sql
â”‚
â”œâ”€â”€ processed_data/
â”‚   â””â”€â”€ [data de execuÃ§Ã£o]/
â”‚       â”œâ”€â”€ csv/
â”‚       â””â”€â”€ sql/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ dbdata/    (verificar dw local)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ img/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE



```

# Processos da DAG

 A DAG Ã© dividida nas seguintes Tasks:

 - **define_pastas_destino**:
 - **processar_csv_transacoes**:
 - **processar_tabelas_banvic**:
 - **carregar_csv_dw_banvic**:
 - **carregar_sql_dw_banvic**:

# Passo-a-passo de execuÃ§Ã£o

 1. Para executar o projeto Ã© necessÃ¡rio realizar o **download** do repositÃ³rio ou    clonar com o seguinte comando no diretÃ³rio desejado:

    ```sh
    git clone https://github.com/leonardo-vargas-de-paula/desafio-LH-ED.git
    ```
    Em seguida entre no diretÃ³rio correspondente ao **root** do projeto *desafio-LH-ED*

 2. Inicialize o Apache Airflow na sua primeira execuÃ§Ã£o:

    ```
    docker-compose up airflow-init
    ```
 3. Inicializar os serviÃ§os:
    ```sh
    docker-compose up -d
    ```
    OBS: Ã‰ importante utilizar o comando **-d** para evitar possÃ­veis travamentos no terminal.

 4. Entre no webserver do Airflow na URL  ``http://localhost:8080`` com login ``airflow`` e senha ``airflow``


5.  Configure as conexÃµes referentes ao banco de dados do Banvic e o Datawarehouse de destino dentro do webserver:
    
       5.1. Localize o botÃ£o de configuraÃ§Ãµes de admin:
        
       <div align="start">
          <img src="img\config.png" width="10%"><br>
       </div>
    
       5.2.Selecione a opÃ§Ã£o *Connections*

       <div align="start">
          <img src="img\connections.png" width="10%"><br>
       </div>
       
       5.3.Clique em *Add Connection*
       
       <div align="start">
          <img src="img\addconnection.png" width="20%"><br>
       
       </div>

       5.4. No campo *Connection ID* escreva **banvic_source_db** para identificar o ID do banco de dados do Banvic e *connection type* igual a **Postgres**.
                                          
       <div align="start">
          <img src="img\dbbanvic1.png" width="30%"><br>
       </div>
    
       5.5. Nos campos das configuraÃ§Ãµes insira as seguintes credenciais:

       - Description: db banvic
       - Host: db
       - Login: data_engineer
       - Password: v3rysecur&pas5w0rd
       - Port: 5432
       - Database: banvic

       <div align="start">
          <img src="img\credenciaisbanvic.png" width="30%"><br>
       </div>

       ApÃ³s isso clique em *save*

       5.6. De forma anÃ¡loga, crie a conexÃ£o com ID **banvic_dw** e *connection type* igual a **Postgres**
       
       <div align="start">
          <img src="img\dwbanvic1.png" width="45%"><br>
       </div>

      5.7. Nos campos das configuraÃ§Ãµes insira as seguintes credenciais:

       - Description: dw banvic
       - Host: dw_local
       - Login: data_engineer
       - Password: v3rysecur&pas5w0rd
       - Port: 5432
       - Database: dw_banvic

       <div align="start">
          <img src="img\credenciaisbanvicdw.png" width="45%"><br>
       </div>
      
      E por fim, clique em *save*. Dessa forma a **DAG** pode funcionar normalmente. Mas antes disso, serÃ¡ necessÃ¡rio ativÃ¡-la.

   6. No menu lateral, clique no botÃ£o *DAGS*:

      <div align="start">
      <img src="img\botaodags.png" width="20%"><br>
      </div>

      6.1.Realize uma busca pela DAG chamada    
      ````processamento_dados_banvic````

   7. Ao encontrar a DAG desejada, clique nela para ter um overview e verificar suas caracterÃ­sticas.

   [adicionar imagem overview]

   8. Clique no switch para ativar a DAG

   [adicionar imagem switch]

   9.  Pronto! A DAG esta agendada para rodar as 4:35 da manhÃ£. Caso deseje rodar manualmente, clique em *trigger*.

   [adicionar imagem botÃ£o trigger]




      


    

